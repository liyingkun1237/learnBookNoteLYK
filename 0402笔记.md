# 0402笔记

## 本周主要任务为：学习 auto_ml mlbox optml tpot模块

## 需达到的目标：使得机器学习平台支持多种算法 包括回归 支持全流程的优化

### 目标的具体细化

------

* 数据预处理
  : 缺失值编码
  : 分类特征编码
  : 均值编码（没有）

* 特征工程
  : 几何变换
  : 符号回归（四个模块皆没有）
  : 决策树变换（四个模块皆没有）
  : 深度学习

* 特征选择
  : PCA
  : 树模型
  : 其余tpot里支持的方法

* 交叉验证
  : 5折交叉验证

* 超参数寻优
  : 网格搜索
  : 随机搜索
  : 贝叶斯优化
  : 遗传算法优化
  : Hyperopt

* stacking 模型融合

------

### 具体的实现思路

1. 弄清楚四个模块都支持哪些功能 将不支持的按其方法添加于其中
2. 需不需要去兼容之前的算法 我建议是可以先独立出来 后续去兼容 主要原因是 之前有日志和文件夹
3. 四个模块皆没有的功能 我建议是自己添加
4. 最佳目标是将pipeline和optimization能联合起来
5. 更难的是能将stacking pipeline optimization联合起来(对付建模大赛可以，机器学习平台就算了)

------

### 统计一下 四个模块都支持的功能

大功能|细化功能|tpot|mlbox|auto_ml|optml
--|--|--|--|--|--
数据预处理|缺失值处理|支持 <br></br>[默认为imputer by median](http://epistasislab.github.io/tpot/api/) <br></br> [同时tpot.builtins.OneHotEncoder模块似乎能将缺失和other区别编码 需进行源码阅读](https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/one_hot_encoder.py)|缺失值编码 分类变量编码 |未知|未知
|特征工程 |特征变换|支持 主要有：<br></br> [sklearn.preprocessing.Binarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html) <br></br> [sklearn.decomposition.FastICA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html) <br></br> [sklearn.cluster.FeatureAgglomeration](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html) <br></br> [sklearn.preprocessing.MaxAbsScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) <br></br> [sklearn.preprocessing.MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) <br></br> [sklearn.preprocessing.Normalizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) <br></br> [sklearn.kernel_approximation.Nystroem](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html) <br></br> [sklearn.decomposition.PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) <br></br> [sklearn.preprocessing.PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) <br></br>[sklearn.kernel_approximation.RBFSampler](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html) <br></br>[sklearn.preprocessing.RobustScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) <br></br>[sklearn.preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) <br></br>[tpot.builtins.ZeroCount](https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/zero_count.py) <br></br>[tpot.builtins.OneHotEncoder](https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/one_hot_encoder.py)|不支持特征的变换 就是有分类变量的编码|未知|未知
特征选择|特征选择|支持 主要有： <br></br> [sklearn.feature_selection.VarianceThreshold](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) <br></br> [sklearn.feature_selection.SelectFromModel](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) <br></br>[按设计逻辑sklearn.feature_selection里的其余方法也应该支持](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)|支持 主要有 {“variance”, “l1”, “rf_feature_importance”}|未知|未知
其它功能|其它|其内置模块有一个[stacking_estimator](https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/stacking_estimator.py)的功能 其思想为新增特征 新增的特征为使用一个BaseEstimator预测出来的predict_proba 属于特征衍生的功能范围|其内置有一个[stacking](http://mlbox.readthedocs.io/en/latest/features.html#stacking) 没有使用成功 具体功能不知道 从其文档而言 是经典版的stacking思想 查看源码后发现 内置写死了stacking的模型 扩展性瞬间没有了|未知|未知
寻优方式|超参数寻优|网格搜索 不支持 <br></br> 随机搜索 不支持 <br></br> 贝叶斯优化 不支持 <br></br> 遗传算法优化 支持 是此模块的基础理论 但不仅仅是模型的超参数 <br></br> Hyperopt 不支持|只支持Tree Parzen Estimator方法|未知|未知
优点||__1.思想上的优势:__   建模的过程可以认为是最终建立出来的模型的一组`超超参数` 即认为不同的建模路径对最终的建模结果是有影响的（本人认为这是肯定的 受限于建模人员的知识边界不同 选择的方法不同都有可能导致此问题）<br></br> __2.支持模型的优势:__<br></br> 其模块是构建于sklearn基础上的应用型模块 其支持了sklearn中的几乎可以说是所有方法 这样就无形中支持了所有的分类算法 xgboost lightgbm catboost都支持 <br></br> __3.开发人员的优势:__ <br></br> 其模块的开发人员为宾夕凡尼亚大学的教授 github上的star个数超3000 总体而言还是值得借鉴学习与使用 其后续计划将扩展功能至支持分布式计算 <br></br> __4.支持自定义的优势:__ <br></br> 支持自定义 [寻优配置](https://github.com/EpistasisLab/tpot/tree/master/tpot/config) 即都在哪些处理过程中寻优 包括有 `Classifiers Preprocesssors Selectors` 支持自定义评价函数scoring 自定义的目标优化函数|__1.分类变量编码方法:__ 其preprocess模块中对于分类变量的编码提供了{“label_encoding”, “dummification”, “random_projection”, “entity_embedding”}这些编码策略 值得学习 
缺点||__1.计算速度:__ <br></br> 由于其思想是基于遗传算法 且数据建模中的流程众多 流程与流程中的组合就会更多 每个流程节点上依然有许多超参数组可以选择 所以总体而言 其速度受到计算复杂度的影响 从根本上就快不了<br></br>  __2.殊途同归性:__ <br></br> 此为本人的定义 大致意思为经过很多种路径的组合然后寻优 最终结果被集成算法给完全cover掉 如我使用中发现 最优的估计器 虽然各种模型都有尝试 但是xgboost或者lightgbm总能最优 虽然说条条大路通罗马 但是高速公路真的就那么几条而已 虽然尝试了好多路径 对于优秀的集成模型而言 前面的种种数据变换 特征选择的操作 对模型最终精度的影响只是杯水车薪而已 这也可能是有经验的建模人员并不会把sklearn中提供的所有模型和方法都尝试一遍 再选择最优的 而是一般都知道了大概的最优路径 直接尝试最优路径 即再不追求模型的极致上 不会把太多的时间浪费在不太优秀的算法上 这就好比说 我们一般上来就会尝试xgboost 而不是先来决策树 再来svm 一个一个的尝试|__1.对theano的强依赖:__ 不安装此模块 无法运行 虽然其只是分类变量编码的地方用到 但即使不使用这个功能 也强制需要安装 导包的时候就报错了<br></br> __2.整体架构上的不足:__ 支持的算法不多 可扩展性不强 且寻优时没有并行化处理
扩展备注||1.特征变换时支持符号回归 需要实现 fit fit_transform方法 <br></br>2.均值编码 使其能处理高基类的分类变量 <br></br>3.对于超参数寻优上 受制于基础思想的原因 网格随机贝叶斯都应该是不行的了 <br></br>4.结合缺点的阐述 集成算法是精度的最终cover者 所以此部分的超参数寻优 应该是最应该实现网格 随机 贝叶斯优化的（这个很难 需要修改大量的源代码）| 1.其对分类变量的编码方法可以单独提取出来学习和应用 <br></br> 2. 其stacking的思想和代码可以进一步实践来作为已用
------

### TPOT学习

``` markdown
# 官方介绍
TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.

The TPOTClassifier performs an intelligent search over machine learning pipelines that can contain supervised `classification models`, `preprocessors`, `feature selection` techniques, and `any other estimator` or `transformer` that follows the scikit-learn API. The TPOTClassifier will also search over the hyperparameters of all objects in the `pipeline`.

By default, TPOTClassifier will search over a broad range of supervised classification algorithms, transformers, and their parameters. However, the algorithms, transformers, and hyperparameters that the TPOTClassifier searches over can be fully customized using the `config_dict` parameter.

# 介绍导读
1. 支持预处理 特征选择
2. 模型支持实现sklearn接口的所有模型
3. 可自定义寻优路径 相当于超参数的范围扩大为流程上每个节点的超参数
```

``` python {cmd=true output="html"}
# 官方提供的案例
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

# step 1 加载数据
digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

# step 2 训练模型
tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)

# step 3 模型评价
print(tpot.score(X_test, y_test))
tpot.export('tpot_mnist_pipeline.py')
```

``` python
# TPOP 中值得学习的一段代码 其可以用来做optml做超参数寻优时的自定义scoring函数

import numpy as np
from sklearn.metrics import make_scorer, SCORERS


def balanced_accuracy(y_true, y_pred):
    """Default scoring function: balanced accuracy.

    Balanced accuracy computes each class' accuracy on a per-class basis using a
    one-vs-rest encoding, then computes an unweighted average of the class accuracies.

    Parameters
    ----------
    y_true: numpy.ndarray {n_samples}
        True class labels
    y_pred: numpy.ndarray {n_samples}
        Predicted class labels by the estimator

    Returns
    -------
    fitness: float
        Returns a float value indicating the individual's balanced accuracy
        0.5 is as good as chance, and 1.0 is perfect predictive accuracy
    """
    all_classes = list(set(np.append(y_true, y_pred)))
    all_class_accuracies = []
    for this_class in all_classes:
        this_class_sensitivity = 0.
        this_class_specificity = 0.
        if sum(y_true == this_class) != 0:
            this_class_sensitivity = \
                float(sum((y_pred == this_class) & (y_true == this_class))) /\
                float(sum((y_true == this_class)))

            this_class_specificity = \
                float(sum((y_pred != this_class) & (y_true != this_class))) /\
                float(sum((y_true != this_class)))

        this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.
        all_class_accuracies.append(this_class_accuracy)

    return np.mean(all_class_accuracies)


SCORERS['balanced_accuracy'] = make_scorer(balanced_accuracy)

```

------

### MLBox学习

``` markdown
# 官网介绍 [链接](http://mlbox.readthedocs.io/en/latest/)

MLBox is a powerful Automated Machine Learning python library. It provides the following features:

1. Fast reading and distributed data preprocessing/cleaning/formatting.
2. Highly robust feature selection and leak detection.
3. Accurate hyper-parameter optimization in high-dimensional space.
4. State-of-the art predictive models for classification and regression (Deep Learning, Stacking, LightGBM,...).
5. Prediction with models interpretation.

# 使用后总结

1. 是基于pipeline的优化 但是其和TPOP的区别为 TPOP为pipeline的路径寻优 而mlbox为用户自定义下的路径内寻优 mlbox支持stacking TPOP里有stacking的transform 但mlbox的为模型融合 TPOP的为特征衍生
2. 寻优的算法为 `Tree Parzen Estimator` 翻译为中文为 `树的概率密度函数的估计` 对于寻优的算法 有一个提及此算法的链接(http://www.johnmyleswhite.com/notebook/2012/07/21/automatic-hyperparameter-tuning-methods/) 可供后续研究与学习 但有一点不足 看源码后可知 不支持并行计算
3. pipeline中支持的过程有
“enc” = “ne” for na encoder # 缺失值编码
“enc” = “ce” for categorical encoder # 分类特征编码 支持的策略 Available strategies = {“label_encoding”, “dummification”, “random_projection”(http://scikit-learn.org/stable/modules/random_projection.html), “entity_embedding”(https://zhuanlan.zhihu.com/p/30267095)}
“enc” = “fs” for feature selector [OPTIONAL] # 特征选择
“enc” = “stck”+str(i) to add layer n°i of meta-features [OPTIONAL] # stacking模型融合
“enc” = “est” for the final estimator # 算法
“param” : a correct associated parameter for each step. Ex: “max_depth” for “enc”=”est”, ... # 算法对应的超参数
```

------

### optml 学习

``` markdown

阅读源码和相关的文档后 发现此模块虽然是专门用于对模型超参数进行寻优的 但是其寻优思想中并没有CV的思想再其中 不可取

```

------

------

### dask-ml 学习

* [dask](https://github.com/dask) 模块简介
  : Parallel computing with task scheduling 这是其模块的介绍 一个分布式的并行计算框架
  其项目下设有 dask-ml dask-kubernetes dask-docker dask-searchCV dask-xgboost 可以说项目扩展很多 从[官网](http://dask.pydata.org/en/latest/)上能得知 其对python常用的numpy pandas sklearn 提供了分布式的并行计算处理方案 很值得使用和学习

------

### [sphnix的自动文档](https://blog.csdn.net/suzyu12345/article/details/52923464)

### [写最好的文档](https://avnpc.com/pages/writing-best-documentation-by-sphinx-github-readthedocs)

### [read the doc的官网](https://docs.readthedocs.io/en/latest/index.html)

### [read the doc 登录网址](https://readthedocs.org/)

### [一篇关于python并行化处理数据预处理的文章](https://yq.aliyun.com/articles/530060)

### [Python · numba 的基本应用](https://zhuanlan.zhihu.com/p/27152060)